{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from matrix import make_confusion_matrix, subplot_confusion_matrix\n",
    "\n",
    "from prl_utils import (\n",
    "    Mode,\n",
    "    read_hdf5,\n",
    "    get_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Constants\n",
    "N_TRAIN_AGENT = 30000\n",
    "N_VAL_AGENT = 3000\n",
    "NUM_TRIAL = 2000\n",
    "mode = Mode.PRL2_intractable\n",
    "\n",
    "def get_latent_labels(data, num_agents, num_trial, mode):\n",
    "  if mode == Mode.PRL2_intractable:\n",
    "    return data['which_state'].to_numpy().astype(np.float32).reshape((num_agents, num_trial))\n",
    "  elif mode == Mode.PRL2:\n",
    "    return data['rpe_history'].to_numpy().astype(np.float32).reshape((num_agents, num_trial))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Read data\n",
    "\n",
    "list_of_train_files = [\n",
    "  #'5000agent_200t_2ParamRL.csv'\n",
    "  'data/to_a0.7_30000agent_2000t_2ParamRL_intractable.csv',\n",
    "]\n",
    "\n",
    "all_train_features = []\n",
    "train_labels = []\n",
    "num_trial = NUM_TRIAL\n",
    "for f in list_of_train_files:\n",
    "  data = read_hdf5(f) if '.h5' in f else pd.read_csv(f)\n",
    "\n",
    "  num_agents = 20000 if '20000agent' in f else N_TRAIN_AGENT\n",
    "  features = get_features(data, num_agents, num_trial, mode=mode)\n",
    "  all_train_features.append(features)\n",
    "\n",
    "  if mode == Mode.PRL2_intractable:\n",
    "    train_labels.append(data['which_state'].to_numpy().astype(np.float32).reshape((num_agents, num_trial)))\n",
    "  elif mode == Mode.PRL2:\n",
    "    train_labels.append(data['rpe_history'].to_numpy().astype(np.float32).reshape((num_agents, num_trial)))\n",
    "\n",
    "all_train_features = tf.concat(all_train_features, 0)\n",
    "train_labels = tf.concat(train_labels, 0)\n",
    "\n",
    "# all_train_features = all_train_features[:500, :NUM_TRIAL, :]\n",
    "# train_labels = train_labels[:500, :NUM_TRIAL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_features, normalized_train_labels = shuffle(all_train_features.numpy(), train_labels.numpy(), random_state=0)\n",
    "if mode == Mode.PRL2_intractable:\n",
    "    normalized_train_labels = tf.keras.utils.to_categorical(normalized_train_labels, num_classes=2)\n",
    "\n",
    "print(train_features.shape, len(normalized_train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    LSTM,\n",
    "    Bidirectional,\n",
    "    GRU,\n",
    "    Concatenate,\n",
    ")\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "RNN = LSTM\n",
    "is_bidirection = False\n",
    "output_dim = 2 if mode == Mode.PRL2_intractable else 1\n",
    "batch_size = 250\n",
    "units = 128\n",
    "learning_rate = 3e-3\n",
    "op_loss = 'binary_crossentropy' #mse\n",
    "decay = 0 # Learning rate decay\n",
    "\n",
    "identifier = f'trnn_{NUM_TRIAL}t_{RNN.__name__}_B{batch_size}_U{units}_{learning_rate}'\n",
    "print(identifier)\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = layers.Input(shape=(None, train_features.shape[2]))\n",
    "\n",
    "if is_bidirection:\n",
    "   encoder = Bidirectional(RNN(units, return_state=True, return_sequences=True))\n",
    "else:\n",
    "    encoder = RNN(units, return_state=True, return_sequences=True)\n",
    "\n",
    "if RNN.__name__ == 'GRU':\n",
    "  if is_bidirection:\n",
    "    encoder_outputs, forward_h, backward_h = encoder(encoder_inputs)\n",
    "    state_h = Concatenate()([forward_h, backward_h])\n",
    "  else:    \n",
    "    encoder_outputs, state_h = encoder(encoder_inputs)\n",
    "\n",
    "  encoder_states = state_h\n",
    "elif is_bidirection:\n",
    "    lstm, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "    state_h = Concatenate()([forward_h, backward_h])\n",
    "    state_c = Concatenate()([forward_c, backward_c])\n",
    "    encoder_states = [state_h, state_c]\n",
    "else:\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "dense_layer_1 = Dense(int(units/2), activation='relu') #'softmax\n",
    "dense_layer_2 = Dense(output_dim, activation='softmax') #'softmax\n",
    "\n",
    "outputs = dense_layer_1(encoder_outputs)\n",
    "outputs = dense_layer_2(outputs)\n",
    "\n",
    "best_model = keras.Model(encoder_inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "best_model.compile(optimizer=optimiser, loss=op_loss)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)]\n",
    "history = best_model.fit(train_features, normalized_train_labels,\n",
    "          batch_size=batch_size,\n",
    "          epochs=100,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(history.history)\n",
    "ax = sns.lineplot(result)\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel(f'{op_loss} loss')\n",
    "\n",
    "plt.savefig(f'{identifier}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model('2000t_B512_U128_0.003_model')\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "\n",
    "prefix = f'to_a0.7_{N_VAL_AGENT}agent_{NUM_TRIAL}t_2ParamRL_intractable'\n",
    "# load dict of arrays\n",
    "features = load(f'{prefix}_features_test.npz')\n",
    "test_features = features['arr_0']\n",
    "\n",
    "labels = load(f'{prefix}_labels_test.npz')\n",
    "test_labels = labels['arr_0']\n",
    "\n",
    "avg_attentive_states = np.mean(test_labels, axis=1)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Inference model\n",
    "# Encode the input as state vectors.\n",
    "output_tokens = best_model.predict(test_features)\n",
    "prediction = np.argmax(output_tokens, axis=-1)\n",
    "prediction[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "t_score = {'avg_attentive_states': [], 'accuracy': []}\n",
    "for i in range(len(test_labels)):\n",
    "  y_true = test_labels[i]\n",
    "  y_pred = prediction[i]\n",
    "  score = accuracy_score(y_true, y_pred)\n",
    "\n",
    "  t_score['avg_attentive_states'].append(avg_attentive_states[i])\n",
    "  t_score['accuracy'].append(score)\n",
    "\n",
    "t_score = pd.DataFrame(t_score)\n",
    "t_score['mean'] = np.mean(t_score['accuracy'])\n",
    "#t_score.to_csv(f'{RESULT_DIR}/{identifier}_accuracy_to_t.csv')\n",
    "\n",
    "t_score['mean'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cf_matrixes = np.zeros((8, 2, 2), dtype=int)\n",
    "for ag in range(len(avg_attentive_states)):\n",
    "    idx = int((avg_attentive_states[ag]*10)%10-2)\n",
    "    cf_matrix = confusion_matrix(test_labels[ag], prediction[ag])\n",
    "    cf_matrixes[idx] += cf_matrix\n",
    "# cf_matrix = confusion_matrix(test_labels.flatten(), prediction.flatten())\n",
    "# cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = 2, 4\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=nrows, ncols=ncols, figsize=(ncols * 5, nrows * 5), sharey=True, sharex=True\n",
    ")\n",
    "\n",
    "avg_attentive_states_range = ['0.2-0.3', '0.3-0.4', '0.4-0.5', '0.5-0.6', '0.6-0.7', '0.7-0.8', '0.8-0.9', '>0.9']\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    ax.set_title(f'avg attentive state percent range {avg_attentive_states_range[idx]}', fontsize=10)\n",
    "    subplot_confusion_matrix(\n",
    "        cf_matrixes[idx], categories=[\"inattentive\", \"attentive\"], percent=\"by_row\", ax=ax,\n",
    "        vmin=0, vmax=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(cf_matrix, categories=['inattentive', 'attentive'], percent='by_row')\n",
    "plt.savefig(f\"binary_{identifier}_confusion_matrix.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
