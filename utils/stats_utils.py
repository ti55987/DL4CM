import numpy as np
from scipy.stats import ttest_rel
from scipy.special import psi

def get_annotations(res, y_key):
    annotations = []
    for sz in res.set_size.unique():
        reverse = res[(res.set_size == sz) & (res.condition == "GOAL:0")]
        std = res[(res.set_size == sz) & (res.condition == "GOAL:1")]
        stat_results = ttest_rel(reverse[y_key], std[y_key], alternative="two-sided")
        annotations.append(
            (("GOAL:0", str(sz)), ("GOAL:1", str(sz)), stat_results.pvalue)
        )
        print(f"GOAL:0 vs GOAL:1: in set size {sz}\n", stat_results.pvalue, "\n")
    return annotations

def calculate_aic(k, log_likelihood):
    """
    Calculate the Akaike Information Criterion (AIC)
    
    Parameters:
    n (int): Number of observations
    k (int): Number of parameters
    log_likelihood (float): Log-likelihood of the model
    
    Returns:
    float: AIC value
    """
    aic = 2 * k - 2 * log_likelihood
    return aic

def calculate_bic(n, k, log_likelihood):
    """
    Calculate the Bayesian Information Criterion (BIC)
    
    Parameters:
    n (int): Number of observations
    k (int): Number of parameters
    log_likelihood (float): Log-likelihood of the model
    
    Returns:
    float: BIC value
    """
    bic = np.log(n) * k - 2 * log_likelihood
    return bic


def spm_BMS(lme, alpha0=None):
    """
    Bayesian model selection for group studies.

    Parameters:
        lme (numpy.ndarray): Array of log model evidences (subjects x models).
        alpha0 (numpy.ndarray): Prior model counts (default: uniform).

    Returns:
        alpha (numpy.ndarray): Vector of model probabilities.
        exp_r (numpy.ndarray): Expectation of the posterior p(r|y).
    """
    Ni, Nk = lme.shape  # number of subjects and models
    c = 1
    cc = 1e-4

    # Prior observations
    if alpha0 is None:
        alpha0 = np.ones(Nk)
    alpha = alpha0.copy()

    # Iterative VB estimation
    while c > cc:
        # Compute posterior belief g(i,k)=q(m_i=k|y_i)
        log_u = np.zeros((Ni, Nk))
        for i in range(Ni):
            for k in range(Nk):
                # Integrate out prior probabilities of models (in log space)
                log_u[i, k] = lme[i, k] + psi(alpha[k]) - psi(np.sum(alpha))
        
        # Exponentiate (to get back to non-log representation)
        u = np.exp(log_u - np.max(log_u, axis=1, keepdims=True))
        
        # Normalisation: sum across all models for i-th subject
        g = u / np.sum(u, axis=1, keepdims=True)
        
        # Expected number of subjects whose data we believe to have been generated by model k
        beta = np.sum(g, axis=0)
        
        # Update alpha
        prev = alpha.copy()
        alpha = alpha0 + beta
        
        # Convergence check
        c = np.linalg.norm(alpha - prev)

    # Compute expectation of the posterior p(r|y)
    exp_r = alpha / np.sum(alpha)

    return alpha, exp_r