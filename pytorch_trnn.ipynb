{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import load\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from matrix import make_confusion_matrix, subplot_confusion_matrix\n",
    "\n",
    "from prl_utils import (\n",
    "    Mode,\n",
    "    read_hdf5,\n",
    "    get_features,\n",
    "    get_labels,\n",
    "    normalize_train_labels,\n",
    "    normalize_val_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN_AGENT = 30000\n",
    "N_VAL_AGENT = 3000\n",
    "NUM_TRIAL = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f'data/vara_{N_TRAIN_AGENT}agent_{NUM_TRIAL}t_2ParamRL_intractable'\n",
    "# load dict of arrays\n",
    "train_features = load(f'{prefix}_features.npz')['arr_0']\n",
    "train_labels = load(f'{prefix}_labels.npz')['arr_0']\n",
    "meta_labels = load(f'{prefix}_pest_labels.npz', allow_pickle=True)['arr_0'].tolist()\n",
    "normalized_train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=2)\n",
    "\n",
    "prefix = f'data/vara_{N_VAL_AGENT}agent_{NUM_TRIAL}t_2ParamRL_intractable'\n",
    "val_features = load(f'{prefix}_features_val.npz')['arr_0']\n",
    "val_labels = load(f'{prefix}_labels_val.npz')['arr_0']\n",
    "val_meta_labels = load(f'{prefix}_pest_labels_val.npz', allow_pickle=True)['arr_0'].tolist()\n",
    "normalized_val_labels = tf.keras.utils.to_categorical(val_labels, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 2000, 3) (3000, 2000, 3)\n"
     ]
    }
   ],
   "source": [
    "meta_labels, name_to_scaler = normalize_train_labels(meta_labels)\n",
    "normalized_meta_labels = meta_labels.reshape(meta_labels.shape[0], meta_labels.shape[1] , 1)\n",
    "normalized_meta_labels = np.swapaxes(np.tile(normalized_meta_labels, NUM_TRIAL), 1, 2)\n",
    "\n",
    "val_meta_labels = normalize_val_labels(val_meta_labels, name_to_scaler)\n",
    "normalized_val_meta_labels = val_meta_labels.reshape(val_meta_labels.shape[0], val_meta_labels.shape[1] , 1)\n",
    "normalized_val_meta_labels = np.swapaxes(np.tile(normalized_val_meta_labels, NUM_TRIAL), 1, 2)\n",
    "\n",
    "print(normalized_meta_labels.shape, normalized_val_meta_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3000, 2000, 46])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_outputs, _, _ = Bidirectional(GRU(23, return_state=True, return_sequences=True))(val_features)\n",
    "seq_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3000, 2000, 46), dtype=float32, numpy=\n",
       "array([[[ 0.01905048,  0.09448703, -0.07114747, ..., -0.18389554,\n",
       "          0.04767518,  0.2348642 ],\n",
       "        [-0.16420776,  0.10160793, -0.16848373, ..., -0.19837473,\n",
       "          0.1066085 ,  0.22861978],\n",
       "        [-0.04389452,  0.15458778, -0.14774536, ..., -0.22222021,\n",
       "          0.06771502,  0.26738757],\n",
       "        ...,\n",
       "        [-0.02933448,  0.08584926, -0.11511254, ..., -0.10244296,\n",
       "          0.0132771 ,  0.1307762 ],\n",
       "        [-0.10104173, -0.04918401, -0.07103021, ...,  0.01587854,\n",
       "         -0.05734932,  0.05737255],\n",
       "        [-0.12650415, -0.09756619, -0.04824181, ...,  0.02272945,\n",
       "         -0.04428459,  0.05094029]],\n",
       "\n",
       "       [[-0.08851406, -0.06745483, -0.02241098, ..., -0.06085907,\n",
       "         -0.02401285,  0.10508896],\n",
       "        [-0.21293856,  0.04585677, -0.14737621, ..., -0.1518887 ,\n",
       "          0.07345036,  0.15780786],\n",
       "        [-0.16807364, -0.02699324, -0.10685254, ..., -0.06884582,\n",
       "         -0.04750039,  0.18017414],\n",
       "        ...,\n",
       "        [-0.09869745,  0.2745986 , -0.23451576, ..., -0.17645735,\n",
       "          0.07969513,  0.16139245],\n",
       "        [-0.2218824 ,  0.1849697 , -0.2483803 , ..., -0.16752231,\n",
       "          0.14213674,  0.0964682 ],\n",
       "        [-0.2816957 ,  0.16736025, -0.26372054, ..., -0.10999002,\n",
       "          0.1024596 ,  0.07011447]],\n",
       "\n",
       "       [[-0.09921362,  0.20160697, -0.21771373, ..., -0.23612988,\n",
       "          0.12813838,  0.20419183],\n",
       "        [-0.13305911,  0.0197761 , -0.14778548, ..., -0.03822869,\n",
       "         -0.06585782,  0.14707437],\n",
       "        [-0.14302   , -0.05866215, -0.10190087, ..., -0.11182128,\n",
       "         -0.00890323,  0.21084566],\n",
       "        ...,\n",
       "        [-0.10545118,  0.23436236, -0.22983909, ..., -0.15106604,\n",
       "          0.03784551,  0.21174335],\n",
       "        [-0.02772848,  0.21662146, -0.16946898, ..., -0.1465191 ,\n",
       "          0.06725444,  0.15212107],\n",
       "        [-0.18720266,  0.148269  , -0.21202509, ..., -0.10999002,\n",
       "          0.1024596 ,  0.07011447]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.01905048,  0.09448703, -0.07114747, ..., -0.1448059 ,\n",
       "          0.00416722,  0.23106197],\n",
       "        [-0.08462346, -0.04222189, -0.05167582, ..., -0.10987145,\n",
       "         -0.01205809,  0.21958566],\n",
       "        [-0.13884646,  0.16512303, -0.22305386, ..., -0.31733137,\n",
       "          0.19917807,  0.30604744],\n",
       "        ...,\n",
       "        [-0.20576286,  0.4045725 , -0.35126978, ..., -0.3041829 ,\n",
       "          0.21417162,  0.27801928],\n",
       "        [-0.21100989,  0.4163302 , -0.35653645, ..., -0.2375029 ,\n",
       "          0.16882013,  0.22488153],\n",
       "        [-0.08199563,  0.31883678, -0.24701044, ..., -0.08524308,\n",
       "          0.02970972,  0.13485534]],\n",
       "\n",
       "       [[ 0.01905048,  0.09448703, -0.07114747, ..., -0.13372627,\n",
       "          0.00638747,  0.1849826 ],\n",
       "        [-0.08462346, -0.04222189, -0.05167582, ..., -0.0683618 ,\n",
       "         -0.03592476,  0.14247844],\n",
       "        [-0.01451839,  0.06039056, -0.08175025, ..., -0.20393918,\n",
       "          0.07963683,  0.17586526],\n",
       "        ...,\n",
       "        [-0.35248354,  0.18416989, -0.29233313, ..., -0.17635942,\n",
       "          0.13631329,  0.09572777],\n",
       "        [-0.3533569 ,  0.1849916 , -0.29280397, ..., -0.11567515,\n",
       "          0.0855515 ,  0.08291287],\n",
       "        [-0.23933321,  0.05321201, -0.20402855, ...,  0.02272945,\n",
       "         -0.04428459,  0.05094029]],\n",
       "\n",
       "       [[-0.08851406, -0.06745483, -0.02241098, ..., -0.12473442,\n",
       "         -0.01125468,  0.23997527],\n",
       "        [-0.13928962,  0.1569173 , -0.21697724, ..., -0.35071456,\n",
       "          0.21801832,  0.33411497],\n",
       "        [-0.1669105 ,  0.28051054, -0.29302892, ..., -0.34192795,\n",
       "          0.21467909,  0.3171947 ],\n",
       "        ...,\n",
       "        [-0.25103003,  0.12389097, -0.22004667, ..., -0.1721099 ,\n",
       "          0.11875552,  0.1562776 ],\n",
       "        [-0.08309648,  0.1720365 , -0.17597066, ..., -0.1465191 ,\n",
       "          0.06725444,  0.15212107],\n",
       "        [-0.20774439,  0.143429  , -0.21606007, ..., -0.10999002,\n",
       "          0.1024596 ,  0.07011447]]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "train_features, normalized_train_labels = val_features, normalized_val_labels\n",
    "train_features, normalized_train_labels, meta_labels = shuffle(train_features, normalized_train_labels, meta_labels, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    LSTM,\n",
    "    Bidirectional,\n",
    "    GRU,\n",
    ")\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "tf.keras.utils.set_random_seed(33)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "OUPUT_DIM = normalized_train_labels.shape[-1]\n",
    "\n",
    "def create_gru_model(\n",
    "    seq_input_dim: int,\n",
    "    param_feature_input_dim: int,\n",
    "    units: int = 70,\n",
    "    dropout: float = 0.2,\n",
    "    dropout1: float = 0.2,\n",
    "    dropout2: float = 0.1,\n",
    "    learning_rate: float = 3e-4,\n",
    "):\n",
    "    intput_b1 = layers.Input(shape=(None, seq_input_dim))\n",
    "    intput_b2 = layers.Input(shape=(None, param_feature_input_dim))\n",
    "\n",
    "    encoder = GRU(units, return_state=True, return_sequences=True) #Bidirectional\n",
    "    encoder_outputs, forward_h = encoder(intput_b1)\n",
    "    param_outputs = Dense(units, activation=\"relu\")(intput_b2)\n",
    "    encoder_outputs = tf.keras.layers.Concatenate(axis=2)(\n",
    "        [encoder_outputs, param_outputs]\n",
    "    )\n",
    "    #rnn_outputs = Dropout(dropout)(rnn_outputs)\n",
    "\n",
    "    # Dense layers\n",
    "    outputs = Dense(int(units/2), activation=\"relu\")(encoder_outputs)\n",
    "    #outputs = Dropout(dropout1)(outputs)\n",
    "    outputs = Dense(int(units / 4), activation=\"relu\")(outputs)\n",
    "    #outputs = Dropout(dropout2)(outputs)\n",
    "    outputs = Dense(OUPUT_DIM, activation=\"softmax\", name=\"trnn\")(outputs)\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs=[intput_b1, intput_b2], outputs=outputs\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "units = 64 #\n",
    "dropout = 0\n",
    "dropout1 = 0\n",
    "dropout2 = 0\n",
    "learning_rate = 3e-3\n",
    "\n",
    "normalized_meta_labels = np.swapaxes(np.tile(meta_labels.reshape(3000, 3 , 1), train_features.shape[1]), 1, 2)\n",
    "\n",
    "identifier = f'vara_B{batch_size}_U{units}_D{dropout}_D{dropout1}_D{dropout2}_{learning_rate}'\n",
    "print(normalized_meta_labels.shape, identifier)\n",
    "\n",
    "best_model = create_gru_model(\n",
    "    seq_input_dim=train_features.shape[2],\n",
    "    param_feature_input_dim=normalized_meta_labels.shape[2],\n",
    "    units=units,\n",
    "    dropout=dropout,\n",
    "    dropout1=dropout1,\n",
    "    dropout2=dropout2,\n",
    "    learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)]\n",
    "history = best_model.fit(\n",
    "          x=[train_features, normalized_meta_labels], #tf.keras.layers.Concatenate(axis=2)([train_features, normalized_meta_labels]), \n",
    "          y=normalized_train_labels,\n",
    "          batch_size=batch_size,\n",
    "          epochs=200,\n",
    "          verbose=2,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2,\n",
    "          #validation_data = (val_features, normalized_val_labels),\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(history.history)\n",
    "ax = sns.lineplot(result)\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel(f'categorial cross entropy loss')\n",
    "\n",
    "#plt.savefig(f'meta_{identifier}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f'data/vara_{N_VAL_AGENT}agent_{NUM_TRIAL}t_2ParamRL_intractable'\n",
    "# load dict of arrays\n",
    "features = load(f'{prefix}_features_test.npz')\n",
    "test_features = features['arr_0']\n",
    "\n",
    "test_labels = load(f'{prefix}_labels_test.npz')['arr_0']\n",
    "test_meta_labels = load(f'{prefix}_pest_labels_test.npz', allow_pickle=True)['arr_0'].tolist()\n",
    "normalized_test_meta_labels = normalize_val_labels(test_meta_labels, name_to_scaler)\n",
    "normalized_test_meta_labels = np.swapaxes(np.tile(normalized_test_meta_labels.reshape(3000, 3 , 1), test_features.shape[1]), 1, 2)\n",
    "\n",
    "all_test_features = tf.keras.layers.Concatenate(axis=2)([test_features, normalized_test_meta_labels])\n",
    "avg_attentive_states = np.mean(test_labels, axis=1)\n",
    "print(all_test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tokens = best_model.predict(all_test_features)\n",
    "prediction = np.argmax(output_tokens, axis=-1)\n",
    "prediction[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "t_score = {'avg_attentive_states': [], 'accuracy': []}\n",
    "for i in range(len(test_labels)):\n",
    "  y_true = test_labels[i]\n",
    "  y_pred = prediction[i]\n",
    "  score = accuracy_score(y_true, y_pred)\n",
    "\n",
    "  t_score['avg_attentive_states'].append(avg_attentive_states[i])\n",
    "  t_score['accuracy'].append(score)\n",
    "\n",
    "t_score = pd.DataFrame(t_score)\n",
    "t_score['mean'] = np.mean(t_score['accuracy'])\n",
    "#t_score.to_csv(f'{RESULT_DIR}/{identifier}_accuracy_to_t.csv')\n",
    "\n",
    "t_score['mean'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cf_matrixes = np.zeros((8, 2, 2), dtype=int)\n",
    "for ag in range(len(avg_attentive_states)):\n",
    "    idx = int((avg_attentive_states[ag]*10)%10-2)\n",
    "    cf_matrix = confusion_matrix(test_labels[ag], prediction[ag])\n",
    "    cf_matrixes[idx] += cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = 2, 4\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=nrows, ncols=ncols, figsize=(ncols * 5, nrows * 5), sharey=True, sharex=True\n",
    ")\n",
    "\n",
    "avg_attentive_states_range = ['0.2-0.3', '0.3-0.4', '0.4-0.5', '0.5-0.6', '0.6-0.7', '0.7-0.8', '0.8-0.9', '>0.9']\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    ax.set_title(f'avg attentive state percent range {avg_attentive_states_range[idx]}', fontsize=10)\n",
    "    subplot_confusion_matrix(\n",
    "        cf_matrixes[idx], categories=[\"inattentive\", \"attentive\"], percent=\"by_row\", ax=ax,\n",
    "        vmin=0, vmax=1\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
