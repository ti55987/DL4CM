{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6afX5qSmiTiu"
      },
      "source": [
        "## Imports\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yppTMGIBiTiu"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'bayesflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m stats\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatetime\u001b[39;00m \u001b[39mimport\u001b[39;00m datetime\n\u001b[0;32m---> 13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbayesflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mbf\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mprl_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     Mode,\n\u001b[1;32m     17\u001b[0m     get_features,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     plot_recovery,\n\u001b[1;32m     25\u001b[0m )\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bayesflow'"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import plotnine as gg\n",
        "from scipy import stats\n",
        "from datetime import datetime\n",
        "import bayesflow as bf\n",
        "\n",
        "from prl_utils import (\n",
        "    Mode,\n",
        "    get_features,\n",
        "    read_hdf5,\n",
        "    get_labels,\n",
        "    padding,\n",
        "    normalize_train_labels,\n",
        "    normalize_val_labels,\n",
        "    get_recovered_parameters,\n",
        "    plot_recovery,\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PMlEqjUMiTix"
      },
      "source": [
        "## Read in data\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHoQwlkUoxlk"
      },
      "outputs": [],
      "source": [
        "# @title Constants\n",
        "DRIVE_DIR = '/content/gdrive/MyDrive/dl4rl'\n",
        "N_TRAIN_AGENT = 3000\n",
        "N_VAL_AGENT = 3000\n",
        "NUM_TRIAL = 2000\n",
        "\n",
        "mode = Mode.PRL2_intractable\n",
        "model_type = \"GRU\" #@@param [\"GRU\", \"LSTM\", \"Transformer\"] {allow-input: true}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title load features\n",
        "from numpy import load\n",
        "\n",
        "prefix = f'data/vara_{N_TRAIN_AGENT}agent_{NUM_TRIAL}t_2ParamRL_intractable'\n",
        "# load dict of arrays\n",
        "train_features = load(f'{prefix}_features.npz')['arr_0']\n",
        "train_labels = load(f'{prefix}_pest_labels.npz', allow_pickle=True)['arr_0'].tolist()\n",
        "normalized_train_labels, name_to_scaler = normalize_train_labels(train_labels)\n",
        "\n",
        "prefix = f'data/vara_{N_VAL_AGENT}agent_{NUM_TRIAL}t_2ParamRL_intractable'\n",
        "val_features = load(f'{prefix}_features_val.npz')['arr_0']\n",
        "val_labels = load(f'{prefix}_pest_labels_val.npz', allow_pickle=True)['arr_0'].tolist()\n",
        "normalized_val_labels = normalize_val_labels(val_labels, name_to_scaler)\n",
        "\n",
        "print(train_features.shape, len(train_labels))\n",
        "OUPUT_DIM = len(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2TIYizx0ZIS0"
      },
      "outputs": [],
      "source": [
        "# @title normalization helper functions\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "def normalize(names_to_labels: dict, full_range_labels: dict):\n",
        "  names = list(names_to_labels.keys())\n",
        "  names.sort()\n",
        "\n",
        "  normalized_labels = []\n",
        "  name_to_scaler = {}\n",
        "  for name in names:\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(full_range_labels[name])\n",
        "    normalized_labels.append(scaler.transform(names_to_labels[name]))\n",
        "    name_to_scaler[name] = scaler\n",
        "\n",
        "  return np.concatenate(normalized_labels, axis=-1), name_to_scaler\n",
        "\n",
        "def mx_normalize(names_to_labels: dict):\n",
        "  names = list(names_to_labels.keys())\n",
        "  names.sort()\n",
        "\n",
        "  normalized_labels = []\n",
        "  name_to_scaler = {}\n",
        "  for name in names:\n",
        "    mmscaler = MinMaxScaler()\n",
        "    if name == 'alpha':\n",
        "      data_min = 0\n",
        "      data_max = 1\n",
        "    elif name == 'beta':\n",
        "      data_min = 0\n",
        "      data_max = 10\n",
        "\n",
        "    # Update with custom range\n",
        "    data_range = data_max - data_min\n",
        "    scale_ = (1 - 0) / data_range\n",
        "    mmscaler.scale_ = scale_\n",
        "    mmscaler.min_ = 0 - data_min * scale_\n",
        "\n",
        "    normalized_labels.append(mmscaler.transform(names_to_labels[name]))\n",
        "    name_to_scaler[name] = mmscaler\n",
        "\n",
        "  return np.concatenate(normalized_labels, axis=-1), name_to_scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2FbUgq7QAjqp"
      },
      "outputs": [],
      "source": [
        "# @title load features\n",
        "from numpy import load\n",
        "\n",
        "prefix = f'{RL_DRIVE_DIR}{DATA_DIR}/with_states/vara_{N_TRAIN_AGENT}agent_{NUM_TRIAL}t_2ParamRL_intractable'\n",
        "# load dict of arrays\n",
        "train_features = load(f'{prefix}_features.npz')['arr_0']\n",
        "train_labels = load(f'{prefix}_pest_labels.npz', allow_pickle=True)['arr_0'].tolist()\n",
        "normalized_train_labels, name_to_scaler = normalize_train_labels(train_labels)\n",
        "\n",
        "prefix = f'{RL_DRIVE_DIR}{DATA_DIR}/with_states/vara_{N_VAL_AGENT}agent_{NUM_TRIAL}t_2ParamRL_intractable'\n",
        "val_features = load(f'{prefix}_features_val.npz')['arr_0']\n",
        "val_labels = load(f'{prefix}_pest_labels_val.npz', allow_pickle=True)['arr_0'].tolist()\n",
        "normalized_val_labels = normalize_val_labels(val_labels, name_to_scaler)\n",
        "\n",
        "print(train_features.shape, len(train_labels))\n",
        "OUPUT_DIM = len(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWoTp-kb2L83"
      },
      "outputs": [],
      "source": [
        "# @title Process data\n",
        "from prl_utils import get_mixed_trials_data\n",
        "\n",
        "target_trial = 2000\n",
        "all_trials = [target_trial]\n",
        "train_features = all_train_features[:, :target_trial, :] #get_mixed_trials_data(all_train_features, all_trials, mode=mode)\n",
        "val_features = all_val_features[:, :target_trial, :] #get_mixed_trials_data(all_val_features, all_trials, mode=mode)\n",
        "\n",
        "# normalize labels\n",
        "normalized_train_labels, name_to_scaler = normalize_train_labels(train_name_to_labels)\n",
        "normalized_val_labels = normalize_val_labels(val_name_to_labels, name_to_scaler)\n",
        "\n",
        "print(train_features.shape, len(val_name_to_labels))\n",
        "OUPUT_DIM = len(val_name_to_labels)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FCZ7rda6nwcM"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3HChN30JwUTN"
      },
      "source": [
        "### BayesFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irZKxnoT9E8M"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "train_features, normalized_train_labels = shuffle(train_features.numpy(), normalized_train_labels, random_state=0)\n",
        "val_features, normalized_val_labels = shuffle(val_features.numpy(), normalized_val_labels, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZDVl9CSrI1I"
      },
      "outputs": [],
      "source": [
        "import bayesflow as bf\n",
        "\n",
        "lstm_units= 36\n",
        "summary_dim= 10\n",
        "num_train_agent = 20000\n",
        "batch_size = 128\n",
        "\n",
        "identifier = f'bi{target_trial}t_L{lstm_units}_S{summary_dim}_B{batch_size}_at{num_train_agent}'\n",
        "#simulations_dict sim_data and prior_draws to be present\n",
        "test_inp = {'sim_data': train_features[:num_train_agent], 'prior_draws': normalized_train_labels[:num_train_agent]}\n",
        "summary_net = bf.networks.SequenceNetwork(lstm_units=lstm_units, summary_dim=summary_dim, bidirectional=True)\n",
        "inference_net = bf.networks.InvertibleNetwork(\n",
        "    num_params=normalized_train_labels.shape[-1],\n",
        "    num_coupling_layers=4,\n",
        "    coupling_settings={\"dense_args\": dict(kernel_regularizer=None), \"dropout\": False},\n",
        ")\n",
        "\n",
        "# summary_rep = summary_net(test_inp[\"sim_data\"]).numpy()\n",
        "# print(\"Shape of simulated data sets: \", test_inp[\"sim_data\"].shape)\n",
        "# print(\"Shape of summary vectors: \", summary_rep.shape)\n",
        "# z, log_det_J = inference_net(test_inp[\"prior_draws\"], summary_rep)\n",
        "# print(\"Shape of latent variables:\", z.numpy().shape)\n",
        "# print(\"Shape of log det Jacobian:\", log_det_J.numpy().shape)\n",
        "\n",
        "amortizer = bf.amortizers.AmortizedPosterior(inference_net, summary_net)\n",
        "trainer = bf.trainers.Trainer(amortizer=amortizer, default_lr=3e-4, checkpoint_path=TEST_RESULT_DIR+'/'+identifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_y5PRkLqxR7"
      },
      "outputs": [],
      "source": [
        "history = trainer.train_offline(\n",
        "    simulations_dict=test_inp,\n",
        "    epochs=200,\n",
        "    batch_size=batch_size,\n",
        "    early_stopping=True,\n",
        "    validation_sims={'sim_data': val_features[:1000], 'prior_draws': normalized_val_labels[:1000]},\n",
        ")\n",
        "\n",
        "fig = bf.diagnostics.plot_losses(history[\"train_losses\"], history[\"val_losses\"])\n",
        "fig.savefig(f'{TEST_RESULT_DIR}/{identifier}_loss.png')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3aWWbBJ1ws7v"
      },
      "source": [
        "### Our vanilla approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RDV8bTtIzSdV"
      },
      "outputs": [],
      "source": [
        "# @title Model definition\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    LSTM,\n",
        "    Bidirectional,\n",
        "    GRU,\n",
        ")\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "\n",
        "tf.keras.utils.set_random_seed(33)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "# Define the LSTM model\n",
        "def create_lstm_model(input_x: int, input_y: int, units: int=70, dropout: float=0.2, dropout1: float=0.2, dropout2: float=0.2, learning_rate: float=1e-3):\n",
        "    activation_func = 'relu'\n",
        "\n",
        "    model = keras.Sequential()\n",
        "    model.add(Bidirectional(LSTM(units, return_sequences=False, input_shape=(input_x, input_y))))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(int(units/2), activation=activation_func))\n",
        "    model.add(Dropout(dropout1))\n",
        "    model.add(Dense(int(units/4), activation=activation_func))\n",
        "    model.add(Dropout(dropout2))\n",
        "    #model.add(Dense(10, activation=activation_func))\n",
        "    model.add(Dense(OUPUT_DIM, activation='linear'))\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss='mse', optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "# Define the LSTM model\n",
        "def create_stacked_lstm_model(input_x: int, input_y: int, units: int, dropout: float, dropout1: float, dropout2: float, learning_rate: float):\n",
        "    activation_func = 'relu'\n",
        "\n",
        "    model = keras.Sequential()\n",
        "    model.add(Bidirectional(LSTM(units=units, return_sequences=True, input_shape=(input_x, input_y))))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Bidirectional(LSTM(units=int(units/2))))\n",
        "    model.add(Dropout(dropout1))\n",
        "    model.add(Dense(units=int(units/4), activation=activation_func))\n",
        "    model.add(Dropout(dropout2))\n",
        "    #model.add(Dense(10, activation=activation_func)) # not in 4rpl\n",
        "    model.add(Dense(units=OUPUT_DIM, activation='linear'))\n",
        "\n",
        "    optimizer = Adam(lr=learning_rate)\n",
        "    model.compile(loss='mse', optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "def create_gru_model(input_x: int, input_y: int, units: int=70, dropout: float=0.2, dropout1: float=0.2, dropout2: float=0.1, learning_rate: float=3e-4, m: Mode=Mode.PRL4):\n",
        "    activation_func = 'relu'\n",
        "    init_scheme = keras.initializers.HeNormal(seed=666)\n",
        "\n",
        "    model = keras.Sequential([layers.Masking(mask_value=-1., input_shape=(input_x, input_y)), GRU(units, return_sequences=False)])\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(int(units/2), activation=activation_func, kernel_initializer=init_scheme))\n",
        "    model.add(Dropout(dropout1))\n",
        "    model.add(Dense(int(units/4), activation=activation_func, kernel_initializer=init_scheme))\n",
        "    model.add(Dropout(dropout2))\n",
        "    #model.add(Dense(10, activation=activation_func))\n",
        "    model.add(Dense(OUPUT_DIM, activation='linear', kernel_initializer=init_scheme))\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss='mse', optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "def create_transformer_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "    learning_rate=0.01,\n",
        "):\n",
        "    def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "        # Normalization and Attention\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "        x = layers.MultiHeadAttention(\n",
        "            key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "        )(x, x)\n",
        "        x = layers.Dropout(dropout)(x)\n",
        "        res = x + inputs\n",
        "\n",
        "        # Feed Forward Part\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "        x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(dropout)(x)\n",
        "        x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "        return x + res\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "\n",
        "    outputs = layers.Dense(OUPUT_DIM, activation=\"linear\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    # key_dim = head_size\n",
        "    # lr = CustomSchedule(key_dim)\n",
        "    # optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "    model.compile(\n",
        "      loss=\"mse\",\n",
        "      optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def get_model(parms):\n",
        "    return create_lstm_model(\n",
        "        parms['input_x'], parms['input_y'], parms['units'], parms['dropout'], parms['dropout1'], parms['dropout2'], parms['learning_rate'])\n",
        "\n",
        "def get_gru_model(parms):\n",
        "    return create_gru_model(\n",
        "        parms['input_x'], parms['input_y'], parms['units'], parms['dropout'], parms['dropout1'], parms['dropout2'], parms['learning_rate'])\n",
        "\n",
        "def get_stacked_bi_model(parms):\n",
        "    return create_stacked_lstm_model(\n",
        "        parms['input_x'], parms['input_y'], parms['units'], parms['dropout'], parms['dropout1'], parms['dropout2'], parms['learning_rate'])\n",
        "\n",
        "def get_transformer_model(parms):\n",
        "    return create_transformer_model(\n",
        "        parms['input_shape'],\n",
        "        parms['head_size'],\n",
        "        parms['num_head'],\n",
        "        parms['ff_dim'],\n",
        "        parms['num_transformer_blocks'],\n",
        "        [parms['mlp_units']],\n",
        "        parms['dropout'],\n",
        "        parms['mlp_dropout'],\n",
        "        parms['learning_rate'],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r6BAyfOaYt-"
      },
      "outputs": [],
      "source": [
        "if model_type == 'Transformer':\n",
        "  units = 186\n",
        "  dropout = 0.3\n",
        "  mlp_dropout = 0.4\n",
        "  learning_rate = 0.003\n",
        "  head_size = 103\n",
        "  num_heads = 2\n",
        "  batch_size = 128\n",
        "  ff_dim = 4 # 10\n",
        "  identifier = f'2PRL_intra_60k_T_H{num_heads}_FD_{ff_dim}_D{dropout}_MD{mlp_dropout}_{learning_rate}'\n",
        "\n",
        "  best_model = create_transformer_model(\n",
        "      train_features.shape[1:],\n",
        "      head_size=head_size,\n",
        "      num_heads=num_heads,\n",
        "      ff_dim=ff_dim,\n",
        "      num_transformer_blocks=2,\n",
        "      mlp_units=[units],\n",
        "      mlp_dropout=mlp_dropout,\n",
        "      dropout=dropout,\n",
        "      learning_rate=learning_rate\n",
        "  )\n",
        "else:\n",
        "  batch_size = 256\n",
        "  units = 256 #\n",
        "  dropout = 0.2\n",
        "  dropout1 = 0.1\n",
        "  dropout2 = 0.02\n",
        "  learning_rate = 3e-4\n",
        "\n",
        "  identifier = f'vara_B{batch_size}_U{units}_D{dropout}_D{dropout1}_D{dropout2}_{learning_rate}'\n",
        "  best_model = create_gru_model(\n",
        "      input_x=train_features.shape[1],\n",
        "      input_y=train_features.shape[2],\n",
        "      units=units,\n",
        "      dropout=dropout,\n",
        "      dropout1=dropout1,\n",
        "      dropout2=dropout2,\n",
        "      learning_rate=learning_rate)\n",
        "identifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxAW7DggjOC0"
      },
      "outputs": [],
      "source": [
        "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.001)\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)]\n",
        "\n",
        "history = best_model.fit(\n",
        "      train_features,\n",
        "      normalized_train_labels,\n",
        "      epochs=200,\n",
        "      batch_size=batch_size,\n",
        "      callbacks=callbacks,\n",
        "      #validation_split=0.1,\n",
        "      validation_data = (val_features, normalized_val_labels),\n",
        "      verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jryHGroFQlr"
      },
      "outputs": [],
      "source": [
        "from prl_utils import (\n",
        "    plot_loss,\n",
        ")\n",
        "plot_loss(history, \"\", \"\")#f\"{TEST_RESULT_DIR}/{identifier}_loss_plot.png\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLvZGiF-55hn"
      },
      "outputs": [],
      "source": [
        "best_model.save(f'{TEST_RESULT_DIR}/{identifier}_model')\n",
        "\n",
        "#Load model data\n",
        "# best_model = tf.keras.models.load_model(f'{TEST_RESULT_DIR}/500t_B256_U128_D0.2_D0.1_D0.01_0.0003_model')\n",
        "# best_model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KPwwDw-API-y"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "S8-rWlhnDr9d"
      },
      "outputs": [],
      "source": [
        "# @title load from test features\n",
        "from numpy import load\n",
        "\n",
        "prefix = f'{RL_DRIVE_DIR}{DATA_DIR}/with_states/vara_{N_VAL_AGENT}agent_{NUM_TRIAL}t_2ParamRL_intractable'\n",
        "# load dict of arrays\n",
        "features = load(f'{prefix}_features_test.npz')\n",
        "test_features = features['arr_0']\n",
        "\n",
        "labels = load(f'{prefix}_pest_labels_test.npz', allow_pickle=True)\n",
        "test_labels = labels['arr_0'].tolist()\n",
        "normalized_test_labels = normalize_val_labels(test_labels, name_to_scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y-gsC8AfzFKf"
      },
      "outputs": [],
      "source": [
        "# @title load from test data\n",
        "dist_name = '' #'_betadistribution_inrange' # _betadistribution\n",
        "#test_data = pd.read_csv(f'{RL_DRIVE_DIR}{DATA_DIR}/{N_VAL_AGENT}agent_{NUM_TRIAL}t_{PARAM}_test.csv')\n",
        "test_data = pd.read_csv(f'{RL_DRIVE_DIR}{DATA_DIR}/with_states/{N_VAL_AGENT}agent_{NUM_TRIAL}t_{PARAM}_test.csv')\n",
        "#test_features = get_test_features_by_trials(test_data, N_VAL_AGENT, NUM_TRIAL, [500], mode=mode)\n",
        "test_features = get_features(test_data, N_VAL_AGENT, NUM_TRIAL, mode=mode)\n",
        "test_name_to_labels = get_labels(test_data, mode)\n",
        "normalized_test_labels = normalize_val_labels(test_name_to_labels, name_to_scaler)\n",
        "\n",
        "print(test_features.shape, normalized_test_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ah97AhJLzdcg"
      },
      "outputs": [],
      "source": [
        "# @title Bayesflow evalutaion\n",
        "#test_sims = trainer.configurator({'sim_data': test_features, 'prior_draws': normalized_test_labels})\n",
        "test_sims = trainer.configurator({'sim_data': test_features[:1000, :target_trial, :], 'prior_draws': normalized_test_labels[:1000]})\n",
        "posterior_draws = amortizer.sample(test_sims, n_samples=1000)\n",
        "\n",
        "f = bf.diagnostics.plot_recovery(\n",
        "    posterior_draws, test_sims[\"parameters\"], param_names=['T', 'alpha', 'beta'], point_agg=np.mean, uncertainty_agg=np.std\n",
        ")\n",
        "f.savefig(f'{TEST_RESULT_DIR}/{identifier}_recovery.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Tbyty4JKOLd"
      },
      "outputs": [],
      "source": [
        "# @title vanilla model: Compute the prediction\n",
        "all_prediction = best_model.predict(test_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4n4zs6LEBqB"
      },
      "outputs": [],
      "source": [
        "recovered = get_recovered_parameters(name_to_scaler, test_labels, all_prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1750Wy_rEJlQ"
      },
      "outputs": [],
      "source": [
        "label = list(test_labels.keys())[0]\n",
        "p = plot_recovery(recovered, label)\n",
        "p.draw()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j7sKrbAZnF1D"
      },
      "source": [
        "### Varied trials evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ0j02-qe7DN"
      },
      "outputs": [],
      "source": [
        "start = N_VAL_AGENT\n",
        "trial_to_param_all_test = {'num_test_trial': []}\n",
        "for i, num_trial in zip(range(start, start*len(ALL_TRIALS)+1, start), ALL_TRIALS):\n",
        "  prediction = all_prediction[i-start:i, :]\n",
        "  recovered = get_recovered_parameters(name_to_scaler, test_name_to_labels, prediction)\n",
        "  trial_to_param_all_test['num_test_trial'].extend([num_trial]*N_VAL_AGENT)\n",
        "  for col in recovered.columns:\n",
        "    if col in trial_to_param_all_test:\n",
        "      trial_to_param_all_test[col].extend(list(recovered[col]))\n",
        "    else:\n",
        "      trial_to_param_all_test[col] = list(recovered[col])\n",
        "\n",
        "trial_to_param_all_test = pd.DataFrame(trial_to_param_all_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzYXI23oLYhJ"
      },
      "outputs": [],
      "source": [
        "trial_to_param_all_test.to_csv(f'{TEST_RESULT_DIR}/{identifier}_predictions.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNufkd1X2HZv"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import spearmanr\n",
        "import seaborn as sns\n",
        "\n",
        "def subplot_recovery_by_trials(data: pd.DataFrame, label: str, trained_trial: str):\n",
        "  true_l, dl_l = f'true_{label}', f'dl_{label}'\n",
        "\n",
        "  g = sns.lmplot(x =true_l, y =dl_l,\n",
        "            col = \"num_test_trial\", col_wrap=3,\n",
        "            line_kws = {\"color\": \"red\"},\n",
        "            data = data)\n",
        "  g.set_axis_labels(f\"True {label}\", f\"Predicted {label}\").set_titles(\"Trials: {col_name}\")\n",
        "\n",
        "  for ax, feature in zip(g.axes.flat, g.col_names):\n",
        "    param_all = data.loc[data.num_test_trial == feature]\n",
        "    r_value, p_value = spearmanr(param_all[true_l], param_all[dl_l])\n",
        "    ax.set_title(ax.get_title() + f', r={r_value:.2f}')\n",
        "\n",
        "  g.fig.suptitle(f'{trained_trial}t GRU {label} recovery', fontsize='xx-large')\n",
        "  plt.tight_layout()\n",
        "  #plt.savefig(f'{TEST_RESULT_DIR}/{trained_trial}t_{label}_recovery.png')\n",
        "\n",
        "\n",
        "\n",
        "for l in test_name_to_labels.keys():\n",
        "  subplot_recovery_by_trials(trial_to_param_all_test, l, target_trial)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYDDW5oxeskp"
      },
      "outputs": [],
      "source": [
        "for t in all_trials:\n",
        "  trial_to_param_all_test = pd.read_csv(f'{TEST_RESULT_DIR}/{t}t_predictions.csv')\n",
        "  for l in test_name_to_labels.keys():\n",
        "    subplot_recovery_by_trials(trial_to_param_all_test, l, t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWfayMkRBcsw"
      },
      "outputs": [],
      "source": [
        "# @title single parameter recovery plot\n",
        "\n",
        "label = list(train_name_to_labels.keys())[0]\n",
        "p = plot_recovery(trial_to_param_all_test.loc[trial_to_param_all_test.num_test_trial == 500], label)\n",
        "#p.draw()\n",
        "fig = p.draw()\n",
        "fig.savefig(f\"{TEST_RESULT_DIR}/{dist_name}_recover_{label}.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WC5hq7p1W19g"
      },
      "outputs": [],
      "source": [
        "def build_parameters(normalized_test_labels, prediction, sorted_lables):\n",
        "  from collections import defaultdict\n",
        "\n",
        "  param_all_test = {}\n",
        "  for idx in range(normalized_test_labels.shape[1]):\n",
        "    l = sorted_lables[idx]\n",
        "    k = f'true_{l}'\n",
        "    param_all_test[k] = normalized_test_labels[:, idx]\n",
        "\n",
        "    k = f'dl_{l}'\n",
        "    param_all_test[k] = prediction[:, idx]\n",
        "\n",
        "  return pd.DataFrame(param_all_test)\n",
        "\n",
        "normalized_param_all_test = build_parameters(normalized_test_labels, prediction, ['alpha', 'beta'])\n",
        "#normalized_param_all_test.to_csv(f\"{TEST_RESULT_DIR}/{identifier}_predict_normalized.csv\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "76fOBwWjP1N7"
      },
      "source": [
        "## GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3S0LTs1nccz"
      },
      "outputs": [],
      "source": [
        "# For grid search\n",
        "!pip install --upgrade keras-hypetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzK8lMQCzXeV"
      },
      "outputs": [],
      "source": [
        "from kerashypetune import KerasBayesianSearch\n",
        "from hyperopt import hp, Trials\n",
        "\n",
        "if model_type == 'Transformer':\n",
        "  model_func = get_transformer_model\n",
        "  param_grid = {\n",
        "      'input_shape': train_features.shape[1:],\n",
        "      'head_size': 4 + hp.randint('head_size', 256),\n",
        "      'num_head': 2 + hp.randint('num_head', 2),\n",
        "      'ff_dim': 4,\n",
        "      'mlp_units': 64, # + hp.randint('mlp_units', 256),\n",
        "      'dropout': hp.uniform('dropout', .1, .4),\n",
        "      'mlp_dropout': hp.uniform('mlp_dropout', .1, .4),\n",
        "      'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.25)),\n",
        "      'epochs': 20,\n",
        "      'num_transformer_blocks': 1 + hp.randint('num_transformer_blocks', 4),\n",
        "      'batch_size': 256,\n",
        "  }\n",
        "else:\n",
        "  model_func = get_gru_model\n",
        "  param_grid = {\n",
        "    'input_x': train_features.shape[1],\n",
        "    'input_y': train_features.shape[2],\n",
        "    'units': 64 + hp.randint('units', 128),\n",
        "    'learning_rate': 0.0079, #hp.loguniform('learning_rate', np.log(3e-4), np.log(0.2)),\n",
        "    'dropout': 0.2, #hp.uniform('dropout', .15, .25),\n",
        "    'dropout1': 0.02, #hp.uniform('dropout1', .01, .1),\n",
        "    'dropout2': 0.02, #hp.uniform('dropout2', .01, .05),\n",
        "    'epochs': 30,\n",
        "    'batch_size': 256,\n",
        "  }\n",
        "\n",
        "kbs = KerasBayesianSearch(model_func, param_grid, monitor='val_loss', greater_is_better=False, n_iter=10, sampling_seed=11)\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=20)]\n",
        "kbs.search(train_features, normalized_train_labels, trials=Trials(), validation_data=(val_features, normalized_val_labels), callbacks=callbacks)\n",
        "print(kbs.best_params)\n",
        "print(kbs.scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-a1HA48vzls"
      },
      "outputs": [],
      "source": [
        "from kerashypetune import KerasGridSearch\n",
        "\n",
        "# Set up hyperparameters to test\n",
        "param_grid = {\n",
        "    'input_x': [train_features.shape[1]],\n",
        "    'input_y': [train_features.shape[2]],\n",
        "    'units': [128, 96],\n",
        "    'dropout': [0.2, 0.1],\n",
        "    'dropout1': [0.1, 0.05],\n",
        "    'dropout2': [0.01, 0.05],\n",
        "    'learning_rate': 0.0085, #3e-4,\n",
        "    'epochs': 25,\n",
        "    'batch_size': 512,\n",
        "}\n",
        "\n",
        "\n",
        "kgs = KerasGridSearch(get_gru_model, param_grid, monitor='val_loss', greater_is_better=False)\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=15)]\n",
        "kgs.search(train_features, normalized_train_labels, validation_data=(val_features, normalized_val_labels), callbacks=callbacks)\n",
        "print(kgs.best_params)\n",
        "print(kgs.scores)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "j7sKrbAZnF1D"
      ],
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
